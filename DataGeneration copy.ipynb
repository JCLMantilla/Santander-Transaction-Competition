{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports are Ready!\n",
      "/Users/juansmacbook/PycharmProjects/Santander_Transaction/Santander-Transaction-Competition\r\n",
      "Hola\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "print('Imports are Ready!')\n",
    "!pwd\n",
    "print(\"Hey!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Loading Datasets\n",
    "\n",
    " As you can see in both sets we contain 20k examples, each containing it's information of its \"ID_code\" and 200 numerical features. It is important to note that the training set contains an aditional column containing the binary target for each example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us load the training and the test set, please wait.\n",
      "Shape of training set: (200000, 202)\n",
      "Shape of testing set: (200000, 201)\n"
     ]
    }
   ],
   "source": [
    "print(\"Let us load the training and the test set, please wait.\")\n",
    "train=pd.read_csv('/Users/juansmacbook/PycharmProjects/Santander_Transaction/Santander-Transaction-Competition/train.csv')\n",
    "test=pd.read_csv('/Users/juansmacbook/PycharmProjects/Santander_Transaction/Santander-Transaction-Competition/test.csv')\n",
    "print(\"Shape of training set: \"+str(train.shape))\n",
    "print(\"Shape of testing set: \"+str(test.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Extracting synthetic examples in the test dataset\n",
    "This competition is interesting because people in the forums discovered that the 50% of the test set is synthetic! The key to distinguish the real testing examples from the fake ones is checking whether the value of a certain feature isn't repeated in another training example. Therefore, only the examples that have *at least one unique value in any of their features* are considered to be real!\n",
    "\n",
    "This of course wasn't discovered by me, this is what the community calls \"Magic\" when it comes to kaggle competitions.\n",
    "\n",
    "In this order of ideas, my next task will be to filter out the fake testing examples and prove to you that this is actually the case. We should be able to notice that 10.000  training examples are real and 10.000 are fake! Then I will elaborate further on why this is really important for the model and what we are going to do after we split the testing set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"Here we define a function that inputs a column from a data frame and returns a list of the values that only appear one time in that data set\"\"\"\n",
    "\n",
    "def unique_values(column):\n",
    "    count=column.value_counts()\n",
    "    return count.index[count==1]\n",
    "\n",
    "\"\"\"We are going to check what examples in test set are real or not by doing value counts, examples whose variables arent repeated in the whole test set are considered to be real!\"\"\"\n",
    "\n",
    "def feature_uniqueness(data_frame): # This function inputs the TEST DATASET and returns its \"extended\" version that says which feature is unique\n",
    "    for column in data_frame.columns[1:]: #Notice that we are dropping the first column since we don't care bout ID\n",
    "        new_column=pd.Series(\n",
    "            data=data_frame[column].isin(unique_values(data_frame[column])).to_numpy(),\n",
    "            name=column+'_unique?'\n",
    "        )\n",
    "        data_frame=pd.concat([data_frame,new_column],axis=1)\n",
    "    return data_frame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  ID_code    var_0    var_1    var_2   var_3    var_4   var_5   var_6  \\\n0  test_0  11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493   \n1  test_1   8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196   \n2  test_2   5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950   \n3  test_3   8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397   \n4  test_4  11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595   \n\n     var_7   var_8  ...  var_190_unique?  var_191_unique?  var_192_unique?  \\\n0  18.2675  2.1337  ...            False            False            False   \n1  18.6316 -4.4131  ...            False            False            False   \n2  20.2537  1.5233  ...            False            False            False   \n3  20.5660  3.3755  ...            False            False            False   \n4  10.6048  2.9890  ...            False            False            False   \n\n   var_193_unique?  var_194_unique?  var_195_unique?  var_196_unique?  \\\n0            False            False            False            False   \n1            False            False            False            False   \n2            False            False            False            False   \n3            False            False            False             True   \n4            False            False            False            False   \n\n   var_197_unique?  var_198_unique?  var_199_unique?  \n0            False            False            False  \n1            False            False            False  \n2            False            False            False  \n3             True            False            False  \n4            False            False            False  \n\n[5 rows x 401 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>...</th>\n      <th>var_190_unique?</th>\n      <th>var_191_unique?</th>\n      <th>var_192_unique?</th>\n      <th>var_193_unique?</th>\n      <th>var_194_unique?</th>\n      <th>var_195_unique?</th>\n      <th>var_196_unique?</th>\n      <th>var_197_unique?</th>\n      <th>var_198_unique?</th>\n      <th>var_199_unique?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test_0</td>\n      <td>11.0656</td>\n      <td>7.7798</td>\n      <td>12.9536</td>\n      <td>9.4292</td>\n      <td>11.4327</td>\n      <td>-2.3805</td>\n      <td>5.8493</td>\n      <td>18.2675</td>\n      <td>2.1337</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test_1</td>\n      <td>8.5304</td>\n      <td>1.2543</td>\n      <td>11.3047</td>\n      <td>5.1858</td>\n      <td>9.1974</td>\n      <td>-4.0117</td>\n      <td>6.0196</td>\n      <td>18.6316</td>\n      <td>-4.4131</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test_2</td>\n      <td>5.4827</td>\n      <td>-10.3581</td>\n      <td>10.1407</td>\n      <td>7.0479</td>\n      <td>10.2628</td>\n      <td>9.8052</td>\n      <td>4.8950</td>\n      <td>20.2537</td>\n      <td>1.5233</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test_3</td>\n      <td>8.5374</td>\n      <td>-1.3222</td>\n      <td>12.0220</td>\n      <td>6.5749</td>\n      <td>8.8458</td>\n      <td>3.1744</td>\n      <td>4.9397</td>\n      <td>20.5660</td>\n      <td>3.3755</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test_4</td>\n      <td>11.7058</td>\n      <td>-0.1327</td>\n      <td>14.1295</td>\n      <td>7.7506</td>\n      <td>9.1035</td>\n      <td>-8.5848</td>\n      <td>6.8595</td>\n      <td>10.6048</td>\n      <td>2.9890</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 401 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This is our new data frame containing 200 extra features that tell us which feature is unique in each column for every training example\"\"\"\n",
    "temporal_df=feature_uniqueness(test)\n",
    "temporal_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proof that 50% of data set is fake!\n",
    "Just as previously mentioned, the only examples that are we going to consider to be non-synthetic will be those have at least one unique value on their features, the ones that don't have any will be considered as fake. We can easily filter out the real and the fake ones via a simple condition.\n",
    "Using this condition we are going to split the training set into real and fake examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_.-._.-._.-._.-._.-._.-._.-._.-._.-._.-._.-._\n",
      "False    100000\n",
      "True     100000\n",
      "dtype: int64\n",
      "_.-._.-._.-._.-._.-._.-._.-._.-._.-._.-._.-._\n",
      "Just as mentioned in the forums, 50% of test data is fake!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"In each example we are checking whether if any of their lats 200 features has a true statement\"\"\"\n",
    "condition=temporal_df[temporal_df.columns[201:]].any(axis=1)\n",
    "\n",
    "print('_.-._.-._.-._.-._.-._.-._.-._.-._.-._.-._.-._')\n",
    "print(temporal_df[temporal_df.columns[201:]].any(axis=1).value_counts())\n",
    "print('_.-._.-._.-._.-._.-._.-._.-._.-._.-._.-._.-._')\n",
    "print('Just as mentioned in the forums, 50% of test data is fake!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\"\"\"Now we can split the dataset in the real part and the fake part\"\"\"\n",
    "\n",
    "real_test_examples=temporal_df[condition==True]\n",
    "fake_test_examples=temporal_df[condition==False]\n",
    "\n",
    "del temporal_df #We delete temporal_df to save memory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ok? half the testing set is fake... so what?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 'MAGIC', and how to break the limits of this Kaggle competition\n",
    "In Kaggle competitions there is something that the community calls \"Magic\", which refers to some tricks or procedures to create competition-winning models. In this specific competition the \"Magic\" to get a top 1% score, as it is discussed in the forums, is the following:\n",
    "\n",
    "1) Filter out the real examples from the testing set via counting unique values (We already did that!)\n",
    "2) Merge the full training set with the real test examples (without the \"var_#_unique?\" features) and do the value counts again as a whole new set, essentially gaining 200 more features again.\n",
    "3) Extracting out the training set, and the real test examples, but each with their 200 additional features. The real test examples must be appended back with the fake test examples (the extended version of them) in order tà get back our testing set.\n",
    "\n",
    "from now on we are going to carry out steps 2 and 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\"\"\"We concatenate the training set with the real training examples into a mixed data frame\"\"\"\n",
    "\n",
    "mixed_df=pd.concat([\n",
    "    train.drop([\"target\"], axis=1), #We drop the target column of the training set because the real_test_examples doesn't have one\n",
    "    real_test_examples.drop(columns=real_test_examples.columns[201:]) #We drop all columns containing if the variables are unique or not, so we can do tha value count again\n",
    "    ],axis=0, ignore_index=True) #Since both data frames contains repeated indices, it is important to reset them to avoid problems"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\"\"\"Once again we use our function feature uniqueness in the combined data frame to extract which variables are real\"\"\"\n",
    "mixed_df=feature_uniqueness(mixed_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/13/z6c3b3d50nnbcf08brkd87dw0000gn/T/ipykernel_98721/655847734.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_train[col]=x_train[col].apply(lambda x: 1 if x==True else 0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Now we are going to crate the actual training sets and testing sets that we are going to use for our model\"\"\"\n",
    "\"\"\"We only have to split the mixed_df data frame again, where the first half is the actual training set, and the other half is part of the training set\"\"\"\n",
    "x_train=mixed_df[:200000]\n",
    "x_test=pd.concat([ mixed_df[200000:], fake_test_examples ], axis=0, ignore_index=True)\n",
    "\n",
    "\"\"\"Now we are going to map all True and False values to 1 and 0 respectively\"\"\"\n",
    "\n",
    "for col in x_test.columns[201:]:\n",
    "    x_test[col]=x_test[col].apply(lambda x: 1 if x==True else 0)\n",
    "\n",
    "for col in x_train.columns[201:]:\n",
    "    x_train[col]=x_train[col].apply(lambda x: 1 if x==True else 0)\n",
    "\n",
    "# REVISAR COMO HACERLO MAS EFICIENTEMENTE!!!!!!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting copy.ipynb       juan_submission.csv       x_test.csv\r\n",
      "DataGeneration copy.ipynb test.csv                  x_train.csv\r\n",
      "README.md                 train.csv\r\n"
     ]
    }
   ],
   "source": [
    "x_test.shape\n",
    "x_test.shape\n",
    "\n",
    "!ls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\"\"\"We save the training sets that are going to be used for the boosting algorithm\"\"\"\n",
    "x_test.to_csv('x_test.csv', index=False)\n",
    "x_train.to_csv('x_train.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}